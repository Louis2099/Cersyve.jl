{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrain affine Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/project/verify_julia_env`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "  \u001b[90m[d8c2afa5] \u001b[39mCersyve v1.0.0-DEV `~/project/Cersyve.jl`\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[587475ba] \u001b[39mFlux v0.13.17\n",
      "  \u001b[90m[f67ccb44] \u001b[39mHDF5 v0.17.2\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.26.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[033835bb] \u001b[39mJLD2 v0.4.53\n",
      "  \u001b[90m[6d061d49] \u001b[39mModelVerification v0.1.0 `/home/jiaxingl/project/ModelVerification.jl#cersyve`\n",
      "  \u001b[90m[85610aed] \u001b[39mNaiveNASflux v2.0.8 `~/project/ModelVerification.jl/onnx_parser/NaiveNASflux`\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[bd45eb3e] \u001b[39mNaiveNASlib v2.0.11\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[d0dd6a25] \u001b[39mONNX v0.2.0\n",
      "  \u001b[90m[2e935253] \u001b[39mONNXNaiveNASflux v0.2.7 `~/project/ModelVerification.jl/onnx_parser/ONNXNaiveNASflux`\n",
      "  \u001b[90m[438e738f] \u001b[39mPyCall v1.96.4\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    }
   ],
   "source": [
    "using Revise\n",
    "using Pkg\n",
    "Pkg.activate(\"/home/jiaxingl/project/verify_julia_env\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Parallel(\n",
       "    vcat,\n",
       "    Chain(\n",
       "      Dense(5 => 3),                    \u001b[90m# 18 parameters\u001b[39m\n",
       "      Dense(3 => 32, relu),             \u001b[90m# 128 parameters\u001b[39m\n",
       "      Dense(32 => 32, relu),            \u001b[90m# 1_056 parameters\u001b[39m\n",
       "    ),\n",
       "    Dense(5 => 2),                      \u001b[90m# 12 parameters\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(34 => 1),                     \u001b[90m# 35 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 10 arrays, \u001b[39m1_249 parameters, 5.871 KiB."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Cersyve\n",
    "using Flux\n",
    "using JLD2\n",
    "using Random\n",
    "\n",
    "# struct FilterX\n",
    "#     W::Matrix  # Weight matrix\n",
    "# end\n",
    "\n",
    "# struct FilterU\n",
    "#     W::Matrix  # Weight matrix\n",
    "# end\n",
    "\n",
    "# function (layer::FilterX)(input::Matrix{Float32})\n",
    "#     return layer.W * input\n",
    "# end\n",
    "\n",
    "# Flux.@functor FilterX  # Make the layer compatible with Flux\n",
    "# function Flux.params(layer::FilterX)\n",
    "#     return Flux.Params([])  # Exclude weights from being trainable\n",
    "# end\n",
    "\n",
    "# # Define a filtering layer for extracting u (indices 9 to 14)\n",
    "\n",
    "\n",
    "# function (layer::FilterU)(input::Matrix{Float32})\n",
    "#     return layer.W * input\n",
    "# end\n",
    "\n",
    "# Flux.@functor FilterU  # Make the layer compatible with Flux\n",
    "# function Flux.params(layer::FilterU)\n",
    "#     return Flux.Params([])  # Exclude weights from being trainable\n",
    "# end\n",
    "\n",
    "# Initialize the fixed weight matrices for filtering\n",
    "function create_filter_matrix(start_idx, end_idx, total_len)\n",
    "    W = zeros(end_idx - start_idx + 1, total_len)\n",
    "    for i in start_idx:end_idx\n",
    "        W[i - start_idx + 1, i] = 1.0\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "function create_parallel_affine_Q(x_dim, u_dim)\n",
    "    # Assume the input has 13 elements: x (0–7), u (8–13)\n",
    "    x_w = create_filter_matrix(1, x_dim, x_dim+u_dim)\n",
    "    x_b = zeros(x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "\n",
    "    u_w = create_filter_matrix(x_dim, x_dim+u_dim-1, x_dim+u_dim)\n",
    "    u_b = zeros(u_dim)\n",
    "    filter_u = Dense(u_w, u_b)\n",
    "    \n",
    "    #Branch1\n",
    "    b1 = Chain(\n",
    "        filter_x,  # First hidden layer (32 neurons, input size is 8 for x)\n",
    "        Dense(x_dim, 32, relu),  # First hidden layer (32 neurons, input size is 8 for x)\n",
    "        Dense(32, 32, relu)  # Second hidden layer (32 neurons)\n",
    "    )\n",
    "\n",
    "    # Define the final output layer (scalar output)\n",
    "    final_layer = Chain(Dense(32 + u_dim, 1))  # Concatenation of x (32) and u (6)\n",
    "\n",
    "    # Complete model\n",
    "    model = Chain(\n",
    "        Parallel(\n",
    "            vcat, \n",
    "            b1,\n",
    "            filter_u\n",
    "        ),\n",
    "        final_layer                       # Compute scalar output\n",
    "    )\n",
    "    return model\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "task = Unicycle\n",
    "value_hidden_sizes = [32, 32]\n",
    "dynamics_hidden_sizes = [32, 32]\n",
    "constraint_hidden_sizes = [16]\n",
    "data_path = joinpath(@__DIR__, \"../data/unicycle_data.jld2\")\n",
    "model_dir = joinpath(@__DIR__, \"../model/unicycle/\")\n",
    "log_dir = joinpath(@__DIR__, \"../log/unicycle/\")\n",
    "seed = 1\n",
    "\n",
    "Random.seed!(seed)\n",
    "\n",
    "# V_model = Cersyve.create_mlp(task.x_dim, 1, value_hidden_sizes)\n",
    "# Q_model = Cersyve.create_mlp(task.x_dim + task.u_dim, 1, value_hidden_sizes)\n",
    "\n",
    "\n",
    "data = JLD2.load(data_path)[\"data\"]\n",
    "f_model = Cersyve.create_mlp(task.x_dim + task.u_dim, task.x_dim, dynamics_hidden_sizes)\n",
    "Flux.loadmodel!(f_model, JLD2.load(joinpath(model_dir, \"f.jld2\"), \"state\"))\n",
    "f_pi_model = Cersyve.create_closed_loop_dynamics_model(\n",
    "    f_model, task.pi_model, data, task.x_low, task.x_high, task.u_dim)\n",
    "\n",
    "h_model = Cersyve.create_mlp(task.x_dim, 1, constraint_hidden_sizes)\n",
    "Flux.loadmodel!(h_model, JLD2.load(joinpath(model_dir, \"h.jld2\"), \"state\"))\n",
    "\n",
    "x_a_low =  [task.x_low; task.u_low]\n",
    "x_a_high = [task.x_high; task.u_high]\n",
    "\n",
    "\n",
    "affine_Q = create_parallel_affine_Q(task.x_dim, task.u_dim)\n",
    "\n",
    "# trainable parameters\n",
    "# println(affine_Q[1][1][2])\n",
    "# println(affine_Q[1][1][3])\n",
    "# println(affine_Q[2])\n",
    "\n",
    "# pretrain_Q(\n",
    "#     affine_Q,\n",
    "#     f_pi_model,\n",
    "#     task.pi_model,\n",
    "#     h_model,\n",
    "#     task.x_low,\n",
    "#     task.x_high;\n",
    "#     penalty=\"APA\",\n",
    "#     space_size=x_a_high - x_a_low,\n",
    "#     apa_coef=1e-4,\n",
    "#     log_dir=log_dir,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Q_Q_next model where Q_next is calculating argmin_u (Q) with interval arithmatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain(Parallel(vcat, Chain(Dense(5 => 3), Dense(3 => 32, relu), Dense(32 => 32, relu)), Dense(5 => 2)), DenseInterval([-2.5848196252384765 -2.2441538891180324 -0.029640842442632934 -1.0459967071975305 0.029323895642413743 0.2491336480656789 0.3311446206204225 0.508050848757809 0.6068861832273464 0.01934963500494938 1.7530919726006564 3.041078597052414 -0.2503978326050154 -0.9469995309488962 1.3602996690080662 0.5612062023558526 1.2793235524402076 1.517217063559812 -0.5110670601092122 0.24033374170978533 0.7029931531963389 -2.049401056841641 0.9025306363275714 -0.7315892175041379 0.31286263262289515 -0.8048340665169595 0.627643925955915 -0.7124762408080163 -0.01248051315316179 -0.6905358345729069 -1.4780301908074667 0.4304669095482064 -0.14959073715810042 0.8390414016976263], [-0.2864204163422486], 3, Float32[-1.0, -1.0], Float32[1.0, 1.0]))\n",
      "PASS 0\n",
      "PASS 1\n",
      "PASS 2\n",
      "32\n",
      "(32,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float32}:\n",
       " -0.19811821\n",
       " -0.30979547"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct ExpandXToXU\n",
    "    x_dim::Int\n",
    "    u_dim::Int\n",
    "end\n",
    "\n",
    "# Define the forward pass for the layer\n",
    "function (layer::ExpandXToXU)(x)\n",
    "    # `x` is the input vector of size `x_dim`\n",
    "    u = rand(layer.u_dim)  # Generate random `u` part\n",
    "    return vcat(x, u)      # Concatenate `x` and `u`\n",
    "end\n",
    "\n",
    "# Example usage\n",
    "function create_expand_xu_layer(x_dim, u_dim)\n",
    "    return ExpandXToXU(x_dim, u_dim)\n",
    "end\n",
    "\n",
    "# Custom Dense Layer for Interval Arithmetic\n",
    "struct DenseInterval\n",
    "    W::AbstractMatrix\n",
    "    b::AbstractVector\n",
    "    x_dim::Int\n",
    "    u_low::AbstractVector\n",
    "    u_up::AbstractVector\n",
    "end\n",
    "\n",
    "function DenseInterval(W, b, x_dim, u_low, u_up)\n",
    "    DenseInterval(W, b, x_dim, u_low, u_up)\n",
    "end\n",
    "\n",
    "# Forward pass for interval arithmetic\n",
    "function (layer::DenseInterval)(xu::AbstractVector)\n",
    "    # Compute lower and upper bounds for each neuron\n",
    "    x_dim = size(xu)[1] - size(layer.u_low)[1]\n",
    "    println(x_dim)\n",
    "    x = xu[1:x_dim]\n",
    "    println(size(x))\n",
    "    W_u = layer.W[:, x_dim+1:end]\n",
    "    W_up = max.(W_u, 0.0)\n",
    "    W_un = min.(W_u, 0.0)\n",
    "    \n",
    "    W_p = layer.W\n",
    "    W_p[:, x_dim+1:end] .= W_up\n",
    "    W_n = layer.W\n",
    "    W_n[:, x_dim+1:end] .= W_un\n",
    "\n",
    "    xu_low = vcat(x, layer.u_low)\n",
    "    xu_high = vcat(x, layer.u_up)\n",
    "    l = W_p * xu_low .+ W_n * xu_high .+ layer.b\n",
    "    return l\n",
    "end\n",
    "\n",
    "# Create the model\n",
    "function create_parallel_affine_Q_interval(x_dim, u_dim, u_low, u_up)\n",
    "    # Assume the input has 13 elements: x (0–7), u (8–13)\n",
    "    x_w = create_filter_matrix(1, x_dim, x_dim+u_dim)\n",
    "    x_b = zeros(x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "\n",
    "    u_w = create_filter_matrix(x_dim, x_dim+u_dim-1, x_dim+u_dim)\n",
    "    u_b = zeros(u_dim)\n",
    "    filter_u = Dense(u_w, u_b)\n",
    "    \n",
    "    # Branch 1\n",
    "    b1 = Chain(\n",
    "        filter_x,\n",
    "        Dense(x_dim, 32, relu),\n",
    "        Dense(32, 32, relu)\n",
    "    )\n",
    "\n",
    "    # Define the interval arithmetic layer for the final output\n",
    "    final_layer_interval = DenseInterval(randn(1, 32 + u_dim), randn(1), task.x_dim, task.u_low, task.u_high)\n",
    "\n",
    "    # Complete model\n",
    "    model = Chain(\n",
    "        Parallel(\n",
    "            vcat, \n",
    "            b1,\n",
    "            filter_u\n",
    "        ),\n",
    "        final_layer_interval  # Perform interval arithmetic here\n",
    "    )\n",
    "    return model\n",
    "end\n",
    "\n",
    "function create_Q_Q_prime(affine_Q, task)\n",
    "    # trainable parameters\n",
    "    # println(affine_Q[1][1][2])\n",
    "    # println(affine_Q[1][1][3])\n",
    "    # println(affine_Q[2])\n",
    "    \n",
    "    # creating Q_prime\n",
    "    affine_Q_interval = create_parallel_affine_Q_interval(task.x_dim, task.u_dim, task.u_low, task.u_high)\n",
    "    println(affine_Q_interval)\n",
    "    # Copy weights and biases from affine_Q to affine_Q_interval\n",
    "    affine_Q_interval[1].layers[1].layers[2].weight .= affine_Q[1].layers[1].layers[2].weight\n",
    "    affine_Q_interval[1].layers[1].layers[2].bias .= affine_Q[1].layers[1].layers[2].bias\n",
    "    affine_Q_interval[1].layers[1].layers[3].weight .= affine_Q[1].layers[1].layers[3].weight\n",
    "    affine_Q_interval[1].layers[1].layers[3].bias .= affine_Q[1].layers[1].layers[3].bias\n",
    "    println(\"PASS 0\")\n",
    "    # Map the final layer to DenseInterval\n",
    "    affine_Q_interval[2].W .= affine_Q[2].layers[1].weight\n",
    "    affine_Q_interval[2].b .= affine_Q[2].layers[1].bias\n",
    "    println(\"PASS 1\")\n",
    "    x_w = create_filter_matrix(1, task.x_dim, task.x_dim + task.u_dim)\n",
    "    x_b = zeros(task.x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "    expand_layer = create_expand_xu_layer(task.x_dim, task.u_dim)\n",
    "    println(\"PASS 2\")\n",
    "    return Chain(Parallel(+,\n",
    "        Chain(affine_Q, Dense(Float32[1; 0;;])),\n",
    "        Chain(filter_x, f_pi_model, expand_layer, affine_Q_interval, Dense(Float32[0; 1;;])),\n",
    "    ))\n",
    "end\n",
    "\n",
    "Q_Q_prime = create_Q_Q_prime(affine_Q, task)\n",
    "# println(Q_Q_prime)\n",
    "\n",
    "input = Float32.(rand(task.x_dim + task.u_dim))\n",
    "Q_Q_prime(input)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
