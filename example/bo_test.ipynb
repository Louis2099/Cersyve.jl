{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrain affine Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/project/verify_julia_env`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "  \u001b[90m[d8c2afa5] \u001b[39mCersyve v1.0.0-DEV `~/project/Cersyve.jl`\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[587475ba] \u001b[39mFlux v0.13.17\n",
      "  \u001b[90m[f67ccb44] \u001b[39mHDF5 v0.17.2\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.26.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[033835bb] \u001b[39mJLD2 v0.4.53\n",
      "  \u001b[90m[6d061d49] \u001b[39mModelVerification v0.1.0 `/home/jiaxingl/project/ModelVerification.jl#cersyve`\n",
      "  \u001b[90m[85610aed] \u001b[39mNaiveNASflux v2.0.8 `~/project/ModelVerification.jl/onnx_parser/NaiveNASflux`\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[bd45eb3e] \u001b[39mNaiveNASlib v2.0.11\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[d0dd6a25] \u001b[39mONNX v0.2.0\n",
      "  \u001b[90m[2e935253] \u001b[39mONNXNaiveNASflux v0.2.7 `~/project/ModelVerification.jl/onnx_parser/ONNXNaiveNASflux`\n",
      "  \u001b[90m[438e738f] \u001b[39mPyCall v1.96.4\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[899adc3e] \u001b[39mTensorBoardLogger v0.1.19\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/home/jiaxingl/project/verify_julia_env\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_parallel_affine_Q (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Cersyve\n",
    "using Flux\n",
    "using JLD2\n",
    "using Random\n",
    "\n",
    "# struct FilterX\n",
    "#     W::Matrix  # Weight matrix\n",
    "# end\n",
    "\n",
    "# struct FilterU\n",
    "#     W::Matrix  # Weight matrix\n",
    "# end\n",
    "\n",
    "# function (layer::FilterX)(input::Matrix{Float32})\n",
    "#     return layer.W * input\n",
    "# end\n",
    "\n",
    "# Flux.@functor FilterX  # Make the layer compatible with Flux\n",
    "# function Flux.params(layer::FilterX)\n",
    "#     return Flux.Params([])  # Exclude weights from being trainable\n",
    "# end\n",
    "\n",
    "# # Define a filtering layer for extracting u (indices 9 to 14)\n",
    "\n",
    "\n",
    "# function (layer::FilterU)(input::Matrix{Float32})\n",
    "#     return layer.W * input\n",
    "# end\n",
    "\n",
    "# Flux.@functor FilterU  # Make the layer compatible with Flux\n",
    "# function Flux.params(layer::FilterU)\n",
    "#     return Flux.Params([])  # Exclude weights from being trainable\n",
    "# end\n",
    "\n",
    "# Initialize the fixed weight matrices for filtering\n",
    "function create_filter_matrix(start_idx, end_idx, total_len)\n",
    "    W = zeros(end_idx - start_idx + 1, total_len)\n",
    "    for i in start_idx:end_idx\n",
    "        W[i - start_idx + 1, i] = 1.0\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "function create_parallel_affine_Q(x_dim, u_dim)\n",
    "    # Assume the input has 13 elements: x (0–7), u (8–13)\n",
    "    x_w = create_filter_matrix(1, x_dim, x_dim+u_dim)\n",
    "    x_b = zeros(x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "\n",
    "    u_w = create_filter_matrix(x_dim, x_dim+u_dim-1, x_dim+u_dim)\n",
    "    u_b = zeros(u_dim)\n",
    "    filter_u = Dense(u_w, u_b)\n",
    "    \n",
    "    #Branch1\n",
    "    b1 = Chain(\n",
    "        filter_x,  # First hidden layer (32 neurons, input size is 8 for x)\n",
    "        Dense(x_dim, 32, relu),  # First hidden layer (32 neurons, input size is 8 for x)\n",
    "        Dense(32, 32, relu)  # Second hidden layer (32 neurons)\n",
    "    )\n",
    "\n",
    "    # Define the final output layer (scalar output)\n",
    "    final_layer = Chain(Dense(32 + u_dim, 1))  # Concatenation of x (32) and u (6)\n",
    "\n",
    "    # Complete model\n",
    "    model = Chain(\n",
    "        Parallel(\n",
    "            vcat, \n",
    "            b1,\n",
    "            filter_u\n",
    "        ),\n",
    "        final_layer                       # Compute scalar output\n",
    "    )\n",
    "    return model\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# task = Unicycle\n",
    "# value_hidden_sizes = [32, 32]\n",
    "# dynamics_hidden_sizes = [32, 32]\n",
    "# constraint_hidden_sizes = [16]\n",
    "# data_path = joinpath(@__DIR__, \"../data/unicycle_data.jld2\")\n",
    "# model_dir = joinpath(@__DIR__, \"../model/unicycle/\")\n",
    "# log_dir = joinpath(@__DIR__, \"../log/unicycle/\")\n",
    "# seed = 1\n",
    "\n",
    "# Random.seed!(seed)\n",
    "\n",
    "# # Q_model = Cersyve.create_mlp(task.x_dim, 1, value_hidden_sizes)\n",
    "# # Q_model = Cersyve.create_mlp(task.x_dim + task.u_dim, 1, value_hidden_sizes)\n",
    "\n",
    "\n",
    "# data = JLD2.load(data_path)[\"data\"]\n",
    "# f_model = Cersyve.create_mlp(task.x_dim + task.u_dim, task.x_dim, dynamics_hidden_sizes)\n",
    "# Flux.loadmodel!(f_model, JLD2.load(joinpath(model_dir, \"f.jld2\"), \"state\"))\n",
    "# f_pi_model = Cersyve.create_closed_loop_dynamics_model(\n",
    "#     f_model, task.pi_model, data, task.x_low, task.x_high, task.u_dim)\n",
    "\n",
    "# h_model = Cersyve.create_mlp(task.x_dim, 1, constraint_hidden_sizes)\n",
    "# Flux.loadmodel!(h_model, JLD2.load(joinpath(model_dir, \"h.jld2\"), \"state\"))\n",
    "\n",
    "# x_a_low =  [task.x_low; task.u_low]\n",
    "# x_a_high = [task.x_high; task.u_high]\n",
    "\n",
    "\n",
    "# affine_Q = create_parallel_affine_Q(task.x_dim, task.u_dim)\n",
    "\n",
    "# trainable parameters\n",
    "# println(affine_Q[1][1][2])\n",
    "# println(affine_Q[1][1][3])\n",
    "# println(affine_Q[2])\n",
    "\n",
    "# pretrain_Q(\n",
    "#     affine_Q,\n",
    "#     f_pi_model,\n",
    "#     task.pi_model,\n",
    "#     h_model,\n",
    "#     task.x_low,\n",
    "#     task.x_high;\n",
    "#     penalty=\"APA\",\n",
    "#     space_size=x_a_high - x_a_low,\n",
    "#     apa_coef=1e-4,\n",
    "#     log_dir=log_dir,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Q_Q_next model where Q_next is calculating argmin_u (Q) with interval arithmatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain(Parallel(vcat, Chain(Dense(5 => 3), Dense(3 => 32, relu), Dense(32 => 32, relu)), Dense(5 => 2)), DenseInterval([1.6318614644804814 -0.4750203677754388 1.8374629813575634 -1.1858311071037995 -0.26274273812587906 -0.5695982462257216 -0.27321555645708484 0.14634685353697807 0.2739900688295851 -0.5071557727533762 1.4204644797023651 -0.8899025038380116 -0.2704353943542073 -0.6577683296151459 -0.4960611390309078 -1.6532945251508397 1.029614670951109 0.3770055695061726 0.01966129609543093 -1.6495535439135751 -0.7966161742358331 0.8244707102628693 0.5431433125818851 -2.4689477858144 -0.8592383103001693 0.3701466512584128 0.06095834176047761 -0.12090820786845707 1.2998409230421089 1.2601360978266014 0.2274674418309091 0.3627061012413202 0.8778780692310526 -0.8808136076851772], [-1.0993995100282379], 3, Float32[-1.0, -1.0], Float32[1.0, 1.0]))\n",
      "PASS 0\n",
      "PASS 1\n",
      "PASS 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float32}:\n",
       " 0.44599584\n",
       " 0.8407302\n",
       " 0.14979398\n",
       " 0.7648264\n",
       " 0.90868866"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct ExpandXToXU\n",
    "    x_dim::Int\n",
    "    u_dim::Int\n",
    "end\n",
    "\n",
    "# Define the forward pass for the layer\n",
    "function (layer::ExpandXToXU)(x)\n",
    "    # `x` is the input vector of size `x_dim`\n",
    "    u = rand(layer.u_dim)  # Generate random `u` part\n",
    "    return vcat(x, u)      # Concatenate `x` and `u`\n",
    "end\n",
    "\n",
    "# Example usage\n",
    "function create_expand_xu_layer(x_dim, u_dim)\n",
    "    return ExpandXToXU(x_dim, u_dim)\n",
    "end\n",
    "\n",
    "# Custom Dense Layer for Interval Arithmetic\n",
    "struct DenseInterval\n",
    "    W::AbstractMatrix\n",
    "    b::AbstractVector\n",
    "    x_dim::Int\n",
    "    u_low::AbstractVector\n",
    "    u_up::AbstractVector\n",
    "end\n",
    "\n",
    "function DenseInterval(W, b, x_dim, u_low, u_up)\n",
    "    DenseInterval(W, b, x_dim, u_low, u_up)\n",
    "end\n",
    "\n",
    "# Forward pass for interval arithmetic\n",
    "function (layer::DenseInterval)(xu::AbstractVector)\n",
    "    # Compute lower and upper bounds for each neuron\n",
    "    x_dim = size(xu)[1] - size(layer.u_low)[1]\n",
    "    println(x_dim)\n",
    "    x = xu[1:x_dim]\n",
    "    println(size(x))\n",
    "    W_u = layer.W[:, x_dim+1:end]\n",
    "    W_up = max.(W_u, 0.0)\n",
    "    W_un = min.(W_u, 0.0)\n",
    "    \n",
    "    W_p = layer.W\n",
    "    W_p[:, x_dim+1:end] .= W_up\n",
    "    W_n = layer.W\n",
    "    W_n[:, x_dim+1:end] .= W_un\n",
    "\n",
    "    xu_low = vcat(x, layer.u_low)\n",
    "    xu_high = vcat(x, layer.u_up)\n",
    "    l = W_p * xu_low .+ W_n * xu_high .+ layer.b\n",
    "    return l\n",
    "end\n",
    "\n",
    "# Create the model\n",
    "function create_parallel_affine_Q_interval(x_dim, u_dim, u_low, u_up)\n",
    "    # Assume the input has 13 elements: x (0–7), u (8–13)\n",
    "    x_w = create_filter_matrix(1, x_dim, x_dim+u_dim)\n",
    "    x_b = zeros(x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "\n",
    "    u_w = create_filter_matrix(x_dim, x_dim+u_dim-1, x_dim+u_dim)\n",
    "    u_b = zeros(u_dim)\n",
    "    filter_u = Dense(u_w, u_b)\n",
    "    \n",
    "    # Branch 1\n",
    "    b1 = Chain(\n",
    "        filter_x,\n",
    "        Dense(x_dim, 32, relu),\n",
    "        Dense(32, 32, relu)\n",
    "    )\n",
    "\n",
    "    # Define the interval arithmetic layer for the final output\n",
    "    final_layer_interval = DenseInterval(randn(1, 32 + u_dim), randn(1), task.x_dim, task.u_low, task.u_high)\n",
    "\n",
    "    # Complete model\n",
    "    model = Chain(\n",
    "        Parallel(\n",
    "            vcat, \n",
    "            b1,\n",
    "            filter_u\n",
    "        ),\n",
    "        final_layer_interval  # Perform interval arithmetic here\n",
    "    )\n",
    "    return model\n",
    "end\n",
    "function create_Q_constraint_model(Q_model, h_model)\n",
    "    x_w = create_filter_matrix(1, task.x_dim, task.x_dim + task.u_dim)\n",
    "    x_b = zeros(task.x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "    return Chain(Parallel(+,\n",
    "        Chain(filter_x, h_model, Dense(Float32[1; 0;;])),\n",
    "        Chain(affine_Q,  Dense(Float32[0; 1;;]))\n",
    "    ))\n",
    "end\n",
    "\n",
    "\n",
    "function create_Q_Q_prime(affine_Q, task)\n",
    "    # trainable parameters\n",
    "    # println(affine_Q[1][1][2])\n",
    "    # println(affine_Q[1][1][3])\n",
    "    # println(affine_Q[2])\n",
    "    \n",
    "    # creating Q_prime\n",
    "    affine_Q_interval = create_parallel_affine_Q_interval(task.x_dim, task.u_dim, task.u_low, task.u_high)\n",
    "    println(affine_Q_interval)\n",
    "    # Copy weights and biases from affine_Q to affine_Q_interval\n",
    "    affine_Q_interval[1].layers[1].layers[2].weight .= affine_Q[1].layers[1].layers[2].weight\n",
    "    affine_Q_interval[1].layers[1].layers[2].bias .= affine_Q[1].layers[1].layers[2].bias\n",
    "    affine_Q_interval[1].layers[1].layers[3].weight .= affine_Q[1].layers[1].layers[3].weight\n",
    "    affine_Q_interval[1].layers[1].layers[3].bias .= affine_Q[1].layers[1].layers[3].bias\n",
    "    println(\"PASS 0\")\n",
    "    # Map the final layer to DenseInterval\n",
    "    affine_Q_interval[2].W .= affine_Q[2].layers[1].weight\n",
    "    affine_Q_interval[2].b .= affine_Q[2].layers[1].bias\n",
    "    println(\"PASS 1\")\n",
    "    x_w = create_filter_matrix(1, task.x_dim, task.x_dim + task.u_dim)\n",
    "    x_b = zeros(task.x_dim)\n",
    "    filter_x = Dense(x_w, x_b)\n",
    "    expand_layer = create_expand_xu_layer(task.x_dim, task.u_dim)\n",
    "    println(\"PASS 2\")\n",
    "    return Chain(Parallel(+,\n",
    "        Chain(affine_Q, Dense(Float32[1; 0;;])),\n",
    "        Chain(filter_x, f_pi_model, expand_layer, affine_Q_interval, Dense(Float32[0; 1;;])),\n",
    "    ))\n",
    "end\n",
    "\n",
    "Q_Q_prime = create_Q_Q_prime(affine_Q, task)\n",
    "# println(Q_Q_prime)\n",
    "\n",
    "input = Float32.(rand(task.x_dim + task.u_dim))\n",
    "# Q_Q_prime(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test affine Q BGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_counterexample_Q (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_min_at_vertices(Q_model, x::Vector{Float32}, u_low::Vector{Float32}, u_high::Vector{Float32}, x_dim::Int)\n",
    "    # Step 1: Compute vertices\n",
    "    vertices = compute_vertices(u_low, u_high)  # Each column is a vertex\n",
    "\n",
    "    # Step 2: Evaluate Q_model at each vertex\n",
    "    min_value = Inf\n",
    "    min_vertex = nothing\n",
    "    for i in 1:size(vertices, 2)\n",
    "        u = vertices[:, i]             # Extract the i-th vertex (u part)\n",
    "        xu = copy(x)                   # Clone x\n",
    "        xu[x_dim+1:end] .= u           # Replace u portion in x\n",
    "        Q_val = Q_model(xu)[1]         # Evaluate Q_model (assume scalar output)\n",
    "        # Step 3: Check if this is the minimum\n",
    "        if Q_val < min_value\n",
    "            min_value = Q_val\n",
    "            min_vertex = u\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return min_value\n",
    "end\n",
    "\n",
    "\n",
    "function compute_vertices(u_low::Vector{Float32}, u_high::Vector{Float32})\n",
    "    n = length(u_low)  # Dimension of the space\n",
    "    vertices = []      # Initialize an empty array to store vertices\n",
    "\n",
    "    # Iterate over all 2^n combinations\n",
    "    for i in 0:(2^n - 1)\n",
    "        vertex = Float32[]  # Initialize a vertex\n",
    "        for j in 1:n\n",
    "            # Check the j-th bit of i to decide low or high\n",
    "            if (i >> (j - 1)) & 1 == 0\n",
    "                push!(vertex, u_low[j])\n",
    "            else\n",
    "                push!(vertex, u_high[j])\n",
    "            end\n",
    "        end\n",
    "        push!(vertices, vertex)  # Add the vertex to the list\n",
    "    end\n",
    "\n",
    "    return reduce(hcat, vertices)  # Return vertices as a matrix (each column is a vertex)\n",
    "end\n",
    "\n",
    "function boundary_guided_search_Q(\n",
    "    task::Any,\n",
    "    x::Matrix{Float32},\n",
    "    x_low::Vector{Float32},\n",
    "    x_high::Vector{Float32},\n",
    "    h_model::Any,\n",
    "    Q_model::Any,\n",
    "    f_pi_model::Any;\n",
    "    pgd_step::Int64 = 10,\n",
    "    pgd_eps::Float64 = 0.1,\n",
    "    pgd_beta::Float64 = 0.0,\n",
    "    backtrack_step::Int64 = 20,\n",
    "    length_discount::Float64 = 0.8,\n",
    "    bound_guide::Bool = true,\n",
    "    direct_discount::Float64 = 0.5,\n",
    "    tol::Float64 = 1e-4,\n",
    ")::Matrix{Float32}\n",
    "\"\"\"\n",
    "1. change the input of h_model and f_pi_model to x[:task.x_dim, :]\n",
    "2. change the properties of verification, but v and v_prime are maintained, represent q and q_prime now\n",
    "\"\"\"\n",
    "    pgd = ones(Bool, size(x, 2))\n",
    "    x_pgd = x\n",
    "    m = zeros(Float32, size(x))\n",
    "\n",
    "    for _ in 1:pgd_step\n",
    "        h = h_model(x_pgd[:task.x_dim, :])[1, :]\n",
    "        v = Q_model(x_pgd)[1, :]\n",
    "        # v_prime = Q_model(f_pi_model(x_pgd[:task.x_dim,:]))[1, :]\n",
    "        v_prime = find_min_at_vertices(Q_model, x_pgd, task.u_low, task.u_high, task.x_dim)\n",
    "        \n",
    "        con = (v .<= tol) .& (h .> -tol)\n",
    "        inv = (v .<= tol) .& (v_prime .> -tol)\n",
    "        \n",
    "        pgd_pgd = pgd[pgd]\n",
    "        pgd_pgd[con .| inv] .= 0\n",
    "        pgd[pgd] = pgd_pgd\n",
    "        x_pgd = x[:, pgd]\n",
    "\n",
    "        con_g = Flux.gradient(x -> sum(h_model(x[:task.x_dim, :])), x_pgd[:, 1:div(size(x_pgd, 2), 2)])[1]\n",
    "        inv_g = Flux.gradient(x -> sum(Q_model(f_pi_model(x[:task.x_dim, :]))), x_pgd[:, size(con_g, 2) + 1:end])[1]\n",
    "        g = hcat(con_g, inv_g) + Float32(pgd_beta) * m[:, pgd]\n",
    "        g ./= sqrt.(sum(g .^ 2, dims=1))\n",
    "\n",
    "        v_g = Flux.gradient(x -> sum(Q_model(x)), x_pgd)[1]\n",
    "        v_g ./= sqrt.(sum(v_g .^ 2, dims=1))\n",
    "\n",
    "        a = sum(g .* v_g, dims=1)\n",
    "        z = a .* g - v_g\n",
    "        z ./= sqrt.(sum(z .^ 2, dims=1))\n",
    "\n",
    "        dirc = zeros(Float32, size(x_pgd))\n",
    "        coef = zeros(Float32, size(x_pgd, 2))\n",
    "        tmp = ones(Bool, size(x_pgd, 2))\n",
    "\n",
    "        for i in 1:backtrack_step + 1\n",
    "            if bound_guide\n",
    "                d_coef = Float32(direct_discount ^ (i - 1))\n",
    "                d = d_coef * g[:, tmp] + (1 - d_coef) * z[:, tmp]\n",
    "                d ./= sqrt.(sum(d .^ 2, dims=1))\n",
    "            else\n",
    "                d = g[:, tmp]\n",
    "            end\n",
    "\n",
    "            l_coef = Float32(length_discount ^ (i - 1))\n",
    "            x_tmp = x_pgd[:, tmp] + Float32.(l_coef * pgd_eps) * d\n",
    "            x_tmp = min.(max.(x_tmp, x_low), x_high)\n",
    "\n",
    "            fea = Q_model(x_tmp)[1, :] .<= tol\n",
    "\n",
    "            dirc[:, tmp] = d\n",
    "\n",
    "            coef_tmp = coef[tmp]\n",
    "            coef_tmp[fea] .= l_coef\n",
    "            coef[tmp] = coef_tmp\n",
    "\n",
    "            tmp_tmp = tmp[tmp]\n",
    "            tmp_tmp[fea] .= 0\n",
    "            tmp[tmp] = tmp_tmp\n",
    "\n",
    "            if maximum(tmp; init=0) == 0\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        dx = reshape(coef, 1, size(x_pgd, 2)) .* dirc\n",
    "        x_pgd = x_pgd + Float32(pgd_eps) * dx\n",
    "        x_pgd = min.(max.(x_pgd, x_low), x_high)\n",
    "        x[:, pgd] = x_pgd\n",
    "\n",
    "        m_pgd = m[:, pgd]\n",
    "        m_pgd[:, .~tmp] = dx[:, .~tmp]\n",
    "        m[:, pgd] = m_pgd\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function filter_counterexample_Q(\n",
    "    task::Any,\n",
    "    x::Matrix{Float32},\n",
    "    h_model::Any,\n",
    "    Q_model::Any,\n",
    "    f_pi_model::Any;\n",
    "    tol::Float64 = 1e-4,\n",
    ")::Tuple{BitVector, BitVector}\n",
    "    h = h_model(x)[1, :]\n",
    "    v = Q_model(x)[1, :]\n",
    "    v_prime = find_min_at_vertices(Q_model, x_pgd, task.u_low, task.u_high, task.x_dim)[1, :]\n",
    "    con = (v .<= tol) .& (h .> -tol)\n",
    "    inv = (v .<= tol) .& (v_prime .> -tol) .& (.~con)\n",
    "    return con, inv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Finetune affine_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finetune_Q (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function finetune_Q(\n",
    "    task::Any,\n",
    "    Q_model::Any,\n",
    "    f_pi_model::Any,\n",
    "    h_model::Any,\n",
    "    x_low::Vector{Float32},\n",
    "    x_high::Vector{Float32};\n",
    "    lr::Float64 = 1e-4,\n",
    "    max_iter::Int64 = 100000,\n",
    "    search_size::Int64 = 1000,\n",
    "    bnd_ratio::Float64 = 0.1,\n",
    "    bnd_ratio_avg::Float64 = 0.9,\n",
    "    min_bnd_ratio::Float64 = 0.01,\n",
    "    max_bnd_ratio::Float64 = 1.0,\n",
    "    bnd_eps::Float64 = 0.1,\n",
    "    search_method::String = \"BGB\",  # \"BGB\" / \"PGD-B\" / \"PBS\"\n",
    "    pgd_step::Int64 = 10,\n",
    "    pgd_eps::Float64 = 0.1,\n",
    "    backtrack_step::Int64 = 20,\n",
    "    length_discount::Float64 = 0.8,\n",
    "    direct_discount::Float64 = 0.5,\n",
    "    tol::Float64 = 1e-4,\n",
    "    capacity::Int64 = 10000,\n",
    "    sample_size::Int64 = 100,\n",
    "    search_stop::Int64 = 1000,\n",
    "    replay::Int64 = 1,\n",
    "    max_skip::Int64 = 1000,\n",
    "    reg_method::Union{String, Nothing} = \"ESR\",  # \"ESR\" / \"RSR\" / nothing\n",
    "    esr_max_con::Union{Int64, Nothing} = nothing,\n",
    "    eps_h::Float64 = 0.01,\n",
    "    eps_v::Float64 = 0.01,\n",
    "    reg_coef::Float64 = 0.1,\n",
    "    log_dir::Union{String, Nothing} = nothing,\n",
    "    eval_every::Int64 = 10,\n",
    "    save_every::Int64 = 1000,\n",
    ")\n",
    "    function update_value_network!(model::Any, loss_fn, opt_state, trainable_params)\n",
    "        # Calculate gradients\n",
    "        loss, grad = Flux.withgradient(loss_fn, model)\n",
    "        value_grad = grad[1][:value_network]\n",
    "        #println(typeof(value_grad))\n",
    "        #println(sizeof(value_grad))\n",
    "        #grad = Flux.gradient(() -> loss_fn, Flux.params(model.value_network))\n",
    "        \n",
    "        Flux.update!(opt_state, model.value_network, value_grad)\n",
    "        return loss\n",
    "    end\n",
    "    \n",
    "\n",
    "    skipped = 0\n",
    "    verified = 0\n",
    "    con_start_values = nothing\n",
    "    inv_start_values = nothing\n",
    "\n",
    "    buffer = Buffer(capacity, length(x_low))\n",
    "\n",
    "    ######################################################\n",
    "    trainable_params = Flux.params(Q_model[1][1][2], Q_model[1][1][3], Q_model[2])\n",
    "    opt_state = Flux.setup(Adam(lr), trainable_params)\n",
    "    #opt_state = Flux.setup(Adam(lr), Q_model[2])\n",
    "    ######################################################\n",
    "    if isnothing(log_dir)\n",
    "        log_dir = joinpath(@__DIR__, \"../log/\")\n",
    "    end\n",
    "    log_path = joinpath(log_dir, \"finetune_\" * Dates.format(Dates.now(), \"yyyymmdd_HHMMSS\"))\n",
    "    logger = TBLogger(log_path)\n",
    "\n",
    "    Q_h_model = create_Q_constraint_model(Q_model, h_model)\n",
    "    Q_Q_prime_model = create_Q_Q_prime(Q_model, task)\n",
    "\n",
    "    for i in ProgressBar(1:max_iter)\n",
    "        if (length(buffer.stored) < search_stop)\n",
    "            x = uniform(x_low, x_high, round(Int64, search_size / bnd_ratio))\n",
    "            v = Q_model(x)[1, :]\n",
    "            x_bnd = x[:, (v .> -bnd_eps) .& (v .<= tol)]\n",
    "            bnd_ratio = bnd_ratio_avg * bnd_ratio + (1 - bnd_ratio_avg) * size(x_bnd, 2) / size(x, 2)\n",
    "            bnd_ratio = clamp(bnd_ratio, min_bnd_ratio, max_bnd_ratio)\n",
    "\n",
    "            if search_method == \"BGB\"\n",
    "                x_pgd = boundary_guided_search_Q(task, x_bnd, x_low, x_high, h_model, Q_model, f_pi_model;\n",
    "                    pgd_step=pgd_step, pgd_eps=pgd_eps, backtrack_step=backtrack_step,\n",
    "                    length_discount=length_discount, bound_guide=true, direct_discount=direct_discount,\n",
    "                    tol=tol)\n",
    "            end\n",
    "            con, inv = filter_counterexample_Q(task, x_pgd, h_model, Q_model, f_pi_model; tol=tol)\n",
    "            ce = con .| inv\n",
    "            push!(buffer, x_pgd[:, ce])\n",
    "\n",
    "            with_logger(logger) do\n",
    "                @info \"finetune\" searched_boundary_states=size(x_bnd, 2) log_step_increment=0\n",
    "                @info \"finetune\" boundary_state_ratio=bnd_ratio log_step_increment=0\n",
    "                @info \"finetune\" searched_constraint_counterexample=sum(con) log_step_increment=0\n",
    "                @info \"finetune\" searched_invariance_counterexample=sum(inv) log_step_increment=0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if length(buffer.stored) > 0\n",
    "            skipped = 0\n",
    "\n",
    "            n = min(sample_size, length(buffer.stored))\n",
    "            x, c = pop!(buffer, n)\n",
    "            con, inv = filter_counterexample_Q(task, x, h_model, Q_model, f_pi_model; tol=tol)\n",
    "            x_con, x_inv = x[:, con], x[:, inv]\n",
    "            c[con .| inv] .= 0\n",
    "            c[.~con .& .~inv] .+= 1\n",
    "            push_idx = c .< replay\n",
    "            push!(buffer, x[:, push_idx], c[push_idx])\n",
    "\n",
    "            n_con, n_inv = size(x_con, 2), size(x_inv, 2)\n",
    "\n",
    "            if !isnothing(reg_method) && (n_con + n_inv > 0) && (\n",
    "                isnothing(esr_max_con) || (n_con + n_inv < esr_max_con))\n",
    "                if reg_method == \"ESR\"\n",
    "                    # entering state regularization\n",
    "                    x_reg = uniform(x_low, x_high, search_size)\n",
    "                    h_reg = h_model(x_reg[:task.x_dim, :])[1, :]\n",
    "                    v_reg = Q_model(x_reg)[1, :]\n",
    "                    v_reg_prime = Q_model(f_pi_model(x_reg[:task.x_dim, :]))[1, :]\n",
    "                    entering = (h_reg .<= -eps_h) .& (v_reg .> 0) .& (\n",
    "                        v_reg .<= eps_v) .& (v_reg_prime .<= -eps_v)\n",
    "                    x_reg = x_reg[:, entering]\n",
    "                    n_reg = size(x_reg, 2)\n",
    "                elseif reg_method == \"RSR\"\n",
    "                    # random state regularization\n",
    "                    x_reg = uniform(x_low, x_high, search_size)\n",
    "                    n_reg = size(x_reg, 2)\n",
    "                end\n",
    "            else\n",
    "                n_reg = 0\n",
    "            end\n",
    "\n",
    "            function value_loss_fn(Q_model)\n",
    "                if n_con > 0\n",
    "                    con_loss = sum(-Q_model(x_con))\n",
    "                else\n",
    "                    con_loss = 0\n",
    "                end\n",
    "\n",
    "                if n_inv > 0\n",
    "                    inv_loss = sum(-Q_model(x_inv) + Q_model(f_pi_model(x_inv[:task.x_dim, :])))\n",
    "                else\n",
    "                    inv_loss = 0\n",
    "                end\n",
    "\n",
    "                if n_reg > 0\n",
    "                    reg_loss = mean(Q_model(x_reg))\n",
    "                else\n",
    "                    reg_loss = 0\n",
    "                end\n",
    "                # println(\"n_con: \", con_loss)\n",
    "                # println(\"n_inv: \", inv_loss)\n",
    "                # println(\"n_reg: \", reg_loss)\n",
    "                loss = (con_loss + inv_loss) / max(n_con + n_inv, 1) + reg_coef * reg_loss\n",
    "                return loss\n",
    "            end\n",
    "            \n",
    "            # regular\n",
    "            loss, grad = Flux.withgradient(value_loss_fn, Q_model)\n",
    "            Flux.update!(opt_state, trainable_params, grad[1])\n",
    "            \n",
    "            ######################################################\n",
    "            # finetune rcppol\n",
    "            # loss, grad = Flux.withgradient(value_loss_fn, Q_model)\n",
    "            # value_grad = grad[1]\n",
    "            # if isnothing(value_grad)\n",
    "            #     println(grad)\n",
    "            #     continue\n",
    "            # end\n",
    "            # value_grad = value_grad[1][2]\n",
    "            # Flux.update!(opt_state, Q_model[2], value_grad)\n",
    "            ######################################################\n",
    "\n",
    "            with_logger(logger) do\n",
    "                @info \"finetune\" sample_size=n log_step_increment=0\n",
    "                @info \"finetune\" value_loss=loss log_step_increment=0\n",
    "                @info \"finetune\" sampled_constraint_counterexample=n_con log_step_increment=0\n",
    "                @info \"finetune\" sampled_invariance_counterexample=n_inv log_step_increment=0\n",
    "                if !isnothing(reg_method)\n",
    "                    @info \"finetune\" regularization_state=n_reg log_step_increment=0\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            skipped += 1\n",
    "        end\n",
    "\n",
    "        if skipped == max_skip\n",
    "            jldsave(joinpath(log_path, \"Q_finetune.jld2\"); state=Flux.state(Q_model))\n",
    "            println(\"----- Verification Starts -----\")\n",
    "            con_res, inv_res = verify_value(x_low, x_high, Q_h_model, Q_Q_prime_model;\n",
    "                con_start_values=con_start_values, inv_start_values=inv_start_values)\n",
    "            println(\"----- Verification Ends -----\")\n",
    "\n",
    "            verified += 1\n",
    "\n",
    "            if (con_res.status == :holds) & (inv_res.status == :holds)\n",
    "                break\n",
    "            else\n",
    "                if con_res.status == :violated\n",
    "                    ce = Float32.(con_res.info[:counter_example])\n",
    "                    push!(buffer, reshape(ce, length(ce), 1))\n",
    "                    println(\"Constraint counterexample: \", ce)\n",
    "                    con_start_values = con_res.info[:verified_bounds][:values]\n",
    "                end\n",
    "                if inv_res.status == :violated\n",
    "                    ce = Float32.(inv_res.info[:counter_example])\n",
    "                    push!(buffer, reshape(ce, length(ce), 1))\n",
    "                    println(\"Invariance counterexample: \", ce)\n",
    "                    inv_start_values = inv_res.info[:verified_bounds][:values]\n",
    "                end\n",
    "                println(\"\")\n",
    "                skipped = 0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        with_logger(logger) do\n",
    "            @info \"finetune\" total_counterexample=length(buffer.stored)\n",
    "            @info \"finetune\" skipped_update=skipped log_step_increment=0\n",
    "            @info \"finetune\" verified_times=verified log_step_increment=0\n",
    "        end\n",
    "\n",
    "        if i % eval_every == 0\n",
    "            fea_rate = mean(Q_model(uniform(x_low, x_high, search_size)) .<= 0)\n",
    "\n",
    "            with_logger(logger) do\n",
    "                @info \"finetune\" predicted_feasible_rate=fea_rate log_step_increment=0\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if i % save_every == 0\n",
    "            jldsave(joinpath(log_path, \"V_finetune.jld2\"); state=Flux.state(Q_model))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.9.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.33.21\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ─ v0.6.73\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/project/verify_julia_env/Project.toml`\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.73\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/project/verify_julia_env/Manifest.toml`\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[93m↑ Zygote v0.6.72 ⇒ v0.6.73\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[33m  ✓ \u001b[39mZygote\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mZygote → ZygoteColorsExt\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mRecursiveArrayTools → RecursiveArrayToolsZygoteExt\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mZygote → ZygoteDistancesExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSparseDiffTools → SparseDiffToolsZygoteExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLabelledArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSciMLBase → ZygoteExt\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mFlux\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffEqBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSciMLNLSolve\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffEqBase → DiffEqBaseDistributionsExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffEqBase → DiffEqBaseUnitfulExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffEqBase → DiffEqBaseZygoteExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSimpleNonlinearSolve\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mNaiveNASflux\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSimpleNonlinearSolve → SimpleNonlinearSolveNNlibExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSymbolicUtils\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mONNXNaiveNASflux\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSymbolics\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSparseDiffTools → SparseDiffToolsSymbolicsExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNonlinearSolve\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOrdinaryDiffEq\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mTaylorIntegration\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mTaylorModels\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mReachabilityAnalysis\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mModelVerification\n",
      "\u001b[33m  ✓ \u001b[39mCersyve\n",
      "  27 dependencies successfully precompiled in 315 seconds. 546 already precompiled.\n",
      "  \u001b[33m12\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n",
      "  \u001b[33m3\u001b[39m dependencies had warnings during precompilation:\u001b[33m\n",
      "┌ \u001b[39mReachabilityAnalysis [1e97bd63-91d1-579d-8e8d-501d2b57c93f]\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition rank(Base.SubArray{N, 2, var\"#s31\", I, L} where L where I where var\"#s31\"<:(SparseArrays.SparseMatrixCSC{Tv, Ti} where Ti<:Integer where Tv)) where {N} in module Arrays at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Arrays/matrix_operations.jl:11 overwritten in module Arrays at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Arrays/matrix_operations.jl:23.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition subtypes(Any, Bool) in module Subtypes at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Subtypes/Subtypes.jl:62 overwritten in module LazySets at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Utils/helper_functions.jl:242.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: method definition for overapproximate at /home/jiaxingl/.julia/packages/ReachabilityAnalysis/HmeX4/src/ReachSets/TemplateReachSet.jl:88 declares type variable VN but does not use it.\u001b[33m\n",
      "│  \u001b[39mWARNING: method definition for _apply_setops at /home/jiaxingl/.julia/packages/ReachabilityAnalysis/HmeX4/src/Discretization/discretization.jl:138 declares type variable BT but does not use it.\u001b[33m\n",
      "│  \u001b[39mWARNING: method definition for _apply_setops at /home/jiaxingl/.julia/packages/ReachabilityAnalysis/HmeX4/src/Discretization/discretization.jl:166 declares type variable BT but does not use it.\u001b[33m\n",
      "│  \u001b[39mWARNING: method definition for reach_homog_GLGM06! at /home/jiaxingl/.julia/packages/ReachabilityAnalysis/HmeX4/src/Algorithms/GLGM06/reach_homog.jl:108 declares type variable MN but does not use it.\u001b[33m\n",
      "│  \u001b[39mWARNING: method definition for reach_homog_GLGM06! at /home/jiaxingl/.julia/packages/ReachabilityAnalysis/HmeX4/src/Algorithms/GLGM06/reach_homog.jl:108 declares type variable VN but does not use it.\u001b[33m\n",
      "└  \u001b[39m\u001b[33m\n",
      "┌ \u001b[39mCersyve [d8c2afa5-f6a5-4ae1-9822-bb2ff2f8c685]\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition rank(Base.SubArray{N, 2, var\"#s6\", I, L} where L where I where var\"#s6\"<:(SparseArrays.SparseMatrixCSC{Tv, Ti} where Ti<:Integer where Tv)) where {N} in module Arrays at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Arrays/matrix_operations.jl:23 overwritten in module Arrays at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Arrays/matrix_operations.jl:11.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition subtypes(Any, Bool) in module LazySets at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Utils/helper_functions.jl:242 overwritten in module Subtypes at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Subtypes/Subtypes.jl:62.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition (::Type{ModelVerification.BetaCrown})(Any) in module ModelVerification at /home/jiaxingl/.julia/packages/ModelVerification/7uE4p/src/solvers/beta-crown.jl:16 overwritten at /home/jiaxingl/.julia/packages/ModelVerification/7uE4p/src/solvers/beta-crown.jl:15.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "└  \u001b[39m\u001b[33m\n",
      "┌ \u001b[39mModelVerification [6d061d49-e389-4b69-b84f-d5b218a5bedb]\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition rank(Base.SubArray{N, 2, var\"#s6\", I, L} where L where I where var\"#s6\"<:(SparseArrays.SparseMatrixCSC{Tv, Ti} where Ti<:Integer where Tv)) where {N} in module Arrays at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Arrays/matrix_operations.jl:23 overwritten in module Arrays at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Arrays/matrix_operations.jl:11.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition subtypes(Any, Bool) in module LazySets at /home/jiaxingl/.julia/packages/LazySets/HlinV/src/Utils/helper_functions.jl:242 overwritten in module Subtypes at /home/jiaxingl/.julia/packages/ReachabilityBase/CsE4N/src/Subtypes/Subtypes.jl:62.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "│  \u001b[39m\u001b[33m\n",
      "│  \u001b[39mWARNING: using ReachabilityAnalysis.flatten in module ModelVerification conflicts with an existing identifier.\u001b[33m\n",
      "│  \u001b[39mWARNING: Method definition (::Type{ModelVerification.BetaCrown})(Any) in module ModelVerification at /home/jiaxingl/.julia/packages/ModelVerification/7uE4p/src/solvers/beta-crown.jl:15 overwritten at /home/jiaxingl/.julia/packages/ModelVerification/7uE4p/src/solvers/beta-crown.jl:16.\u001b[33m\n",
      "│  \u001b[39m  ** incremental compilation may be fatally broken for this module **\u001b[33m\n",
      "└  \u001b[39m\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/project/verify_julia_env/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/project/verify_julia_env`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain(Parallel(vcat, Chain(Dense(5 => 3), Dense(3 => 32, relu), Dense(32 => 32, relu)), Dense(5 => 2)), DenseInterval([-1.134252690960504 1.8071933461740044 -1.084826307447385 -0.14248380595684645 0.21401319233950164 0.13397969342805424 -0.20270264541701283 -0.30583663617014767 -1.9035464029806273 0.4233188451796111 -0.9586185563389916 0.4602760295043951 1.1906650690496055 0.31106251734568724 -0.3968389238872683 -1.1776500982708564 0.6617237149947264 -1.4174000527609407 0.7650605900951314 -0.08636494941797362 2.0015052913450697 1.863251721921747 -0.6175519164453656 1.935888898343217 -1.3745543091659702 0.5353875035482472 -0.2975578765517036 -0.7782751502039986 0.08543970694553495 -1.3823862741132211 -0.3401546523679992 -0.11686333411877826 3.8086629194543726 0.49905350751278593], [-0.2261995399891188], 3, Float32[-1.0, -1.0], Float32[1.0, 1.0]))\n",
      "PASS 0\n",
      "PASS 1\n",
      "PASS 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39msetup found no trainable parameters in this model\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optimisers ~/.julia/packages/Optimisers/er7zv/src/interface.jl:28\u001b[39m\n",
      "0.0%┣                                           ┫ 0/100.0k [00:00<00:-1, -0s/it]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `uniform` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `uniform` not defined",
      "",
      "Stacktrace:",
      " [1] finetune_Q(task::Module, Q_model::Chain{Tuple{Parallel{typeof(vcat), Tuple{Chain{Tuple{Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}}}, f_pi_model::Chain{Tuple{Parallel{typeof(+), Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Chain{Tuple{Parallel{typeof(+), Tuple{Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}}}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, h_model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, x_low::Vector{Float32}, x_high::Vector{Float32}; lr::Float64, max_iter::Int64, search_size::Int64, bnd_ratio::Float64, bnd_ratio_avg::Float64, min_bnd_ratio::Float64, max_bnd_ratio::Float64, bnd_eps::Float64, search_method::String, pgd_step::Int64, pgd_eps::Float64, backtrack_step::Int64, length_discount::Float64, direct_discount::Float64, tol::Float64, capacity::Int64, sample_size::Int64, search_stop::Int64, replay::Int64, max_skip::Int64, reg_method::String, esr_max_con::Nothing, eps_h::Float64, eps_v::Float64, reg_coef::Float64, log_dir::String, eval_every::Int64, save_every::Int64)",
      "   @ Main ./In[9]:73",
      " [2] top-level scope",
      "   @ In[16]:65"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "using Dates\n",
    "# Pkg.add(\"TensorBoardLogger\")\n",
    "Pkg.add(\"Printf\")              # Standard library, no need to add in most cases\n",
    "Pkg.add(\"ProgressBars\")\n",
    "Pkg.add(\"Random\")              # Standard library, no need to add in most cases\n",
    "Pkg.add(\"Statistics\")          # Standard library, no need to add in most cases\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"TensorBoardLogger\")\n",
    "Pkg.add(\"Zygote\")\n",
    "Pkg.add(\"PyCall\")\n",
    "Pkg.add(\"HDF5\")\n",
    "Pkg.activate(\"/home/jiaxingl/project/verify_julia_env\")\n",
    "using Dates\n",
    "using Flux\n",
    "using JLD2\n",
    "using LazySets\n",
    "using LinearAlgebra\n",
    "using Logging\n",
    "using ModelVerification\n",
    "using Printf\n",
    "using ProgressBars\n",
    "using Random\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using TensorBoardLogger\n",
    "using Zygote\n",
    "using PyCall\n",
    "using HDF5\n",
    "using TensorBoardLogger\n",
    "using ProgressBars\n",
    "# Pkg.status()\n",
    "include(\"/home/jiaxingl/BoArchive/project/Cersyve.jl/src/buffer.jl\")\n",
    "\n",
    "\n",
    "\n",
    "task = Unicycle\n",
    "value_hidden_sizes = [32, 32]\n",
    "dynamics_hidden_sizes = [32, 32]\n",
    "constraint_hidden_sizes = [16]\n",
    "data_path = joinpath(@__DIR__, \"../data/unicycle_data.jld2\")\n",
    "model_dir = joinpath(@__DIR__, \"../model/unicycle/\")\n",
    "log_dir = joinpath(@__DIR__, \"../log/unicycle/\")\n",
    "seed = 1\n",
    "\n",
    "Random.seed!(seed)\n",
    "\n",
    "\n",
    "data = JLD2.load(data_path)[\"data\"]\n",
    "f_model = Cersyve.create_mlp(task.x_dim + task.u_dim, task.x_dim, dynamics_hidden_sizes)\n",
    "Flux.loadmodel!(f_model, JLD2.load(joinpath(model_dir, \"f.jld2\"), \"state\"))\n",
    "f_pi_model = Cersyve.create_closed_loop_dynamics_model(\n",
    "    f_model, task.pi_model, data, task.x_low, task.x_high, task.u_dim)\n",
    "\n",
    "h_model = Cersyve.create_mlp(task.x_dim, 1, constraint_hidden_sizes)\n",
    "Flux.loadmodel!(h_model, JLD2.load(joinpath(model_dir, \"h.jld2\"), \"state\"))\n",
    "\n",
    "x_a_low =  [task.x_low; task.u_low]\n",
    "x_a_high = [task.x_high; task.u_high]\n",
    "\n",
    "\n",
    "affine_Q = create_parallel_affine_Q(task.x_dim, task.u_dim)\n",
    "\n",
    "\n",
    "finetune_Q(\n",
    "    task, \n",
    "    affine_Q,\n",
    "    f_pi_model,\n",
    "    h_model,\n",
    "    x_a_low,\n",
    "    x_a_high;\n",
    "    log_dir=log_dir,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
